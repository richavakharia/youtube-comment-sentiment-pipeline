# -*- coding: utf-8 -*-
"""pyspark_sentiment_analysis2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hnS3jXNkvGp0c39dB9CT5ashZGIavpkV

## Environment Setup
"""

# !pip install google-cloud-storage

# !pip install pandas google-cloud-storage gcsfs

# already installed on vm
# !apt-get update -y && apt-get install -y openjdk-17-jdk-headless

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("cs131-youtube-sprint6")
    .config("spark.sql.shuffle.partitions", "8")
    .config("spark.driver.memory", "4g")
    .getOrCreate()
)

print("Spark version:", spark.version)

# !java -version

"""# Data Prep

Data preparation for your final deliverable (analysis, modeling):
Use PySpark meaningfully.

Show non-trivial transforms (e.g., groupBy/agg, join, or window).
"""

from google.cloud import storage
import pandas as pd
import io

bucket = "cs131-tagteam"
yt_comments_path = f"gs://{bucket}/data/cleaned_full_dataset.csv"

from pyspark.sql import functions as F

# df = pd.read_csv(yt_comments_path)
# df.head()
df = (spark.read.option("header", True).csv(yt_comments_path))
df.show(5)
print("Row count:", df.count())
df.printSchema()

df.show(10)

# data cleanup

# remove null comment_id's
# remove null video id's? and video id's greater than 10 characters/with space characters
# remove video with null authors
df.createOrReplaceTempView("yt_comments")

cleaned_df = spark.sql("""
SELECT *
FROM yt_comments
WHERE comment_id IS NOT NULL
  AND video_id IS NOT NULL
  AND video_id NOT RLIKE '\\s'
  AND author_display_name IS NOT NULL
""")

cleaned_df.show(5)
print("Row count:", cleaned_df.count())

# additional transformations
# make a replies specific df and a parent specific df (only comments that are replies and then only comments that have replies)
cleaned_df.createOrReplaceTempView("yt_comments_clean")

comment_replies = spark.sql("""
SELECT *
FROM yt_comments_clean
WHERE is_reply IS NOT NULL AND is_reply != 0 AND parent_id NOT RLIKE '\\s'
  OR parent_id IS NOT NULL AND parent_id != 0 AND parent_id NOT RLIKE '\\s'
""")

comment_replies.show(5)
print("Row count:", comment_replies.count())

# group by, join, aggregations

# per video statistics
video_stats = spark.sql("""
SELECT
  video_id,
  COUNT(comment_id) AS total_comments,
  SUM(CASE WHEN is_reply = false THEN 1 ELSE 0 END) AS top_level_comments,
  SUM(CASE WHEN is_reply = true THEN 1 ELSE 0 END) AS replies,
  AVG(like_count) AS avg_comment_likes,
  MAX(like_count) AS max_comment_likes
FROM yt_comments_clean
GROUP BY video_id
ORDER BY total_comments DESC
""")
video_stats.show(10)
print("Row count:", video_stats.count())

# group by channel id
# sum likes, count total comments
channel_group = spark.sql("""
SELECT
  channel_id,
  COUNT(DISTINCT video_id) AS num_videos,
  COUNT(comment_id) AS total_comments,
  AVG(like_count) AS avg_like_per_comment
FROM yt_comments_clean
GROUP BY channel_id
ORDER BY total_comments DESC
""")
channel_group.show(10)
print("Row count:", channel_group.count())

# group by author display name
# sum likes, replies
comment_author_groups = spark.sql("""
SELECT
  author_display_name,
  COUNT(comment_id) AS total_comments,
  SUM(like_count) AS total_likes,
  AVG(like_count) AS avg_likes_per_comment
FROM yt_comments_clean
GROUP BY author_display_name
ORDER BY total_comments DESC
""")
comment_author_groups.show(10)
print("Row count:", comment_author_groups.count())

# group by published date
# aggregate likes by date?
publish_date_groups = spark.sql("""
SELECT
  DATE(published_at) AS date,
  COUNT(*) AS comment_count,
  AVG(like_count) AS avg_likes
FROM yt_comments_clean
GROUP BY DATE(published_at)
ORDER BY date
""")
publish_date_groups.show(10)
print("Row count:", publish_date_groups.count())

"""# Rebuild PA2 / PA3 tables


- STEP 3A — Top-N and Skinny Tables
- STEP 3B — Ratios, Buckets"
- STEP 3C — quality filters, month-level"
- STEP 3D — 2 frequency tables
- STEP 4 — one runnable entry script or notebook


"""

top_channel_id = spark.sql("""
SELECT
  channel_id,
  COUNT(DISTINCT video_id) AS num_videos
FROM yt_comments_clean
WHERE channel_id LIKE 'UC%'
GROUP BY channel_id
ORDER BY num_videos DESC
""")
top_channel_id.show(10)
print("Row count:", top_channel_id.count())

# frequency of videos published by date
freq_dates = spark.sql("""
SELECT
  DATE(published_at) AS date,
  COUNT(DISTINCT video_id) AS num_videos
FROM yt_comments_clean
WHERE published_at IS NOT NULL AND channel_id LIKE 'UC%'
GROUP BY DATE(published_at)
ORDER BY num_videos DESC
""")
freq_dates.show(20)

# Top-N entity list WITH counts (top videos by comment count)
from pyspark.sql.window import Window

video_window = Window.orderBy(F.col("total_comments").desc())

top_videos = spark.sql("""
SELECT
  video_id,
  COUNT(comment_id) AS total_comments
FROM yt_comments_clean
GROUP BY video_id
""")

top_videos = top_videos.withColumn("rank", F.row_number().over(video_window))

top_videos.show(10)

# top-30 videos by comment count, select only top 30
top_30_videos = spark.sql("""
SELECT
  video_id,
  COUNT(comment_id) AS total_comments
FROM yt_comments_clean
GROUP BY video_id
ORDER BY total_comments DESC
LIMIT 30
""")
top_30_videos = top_30_videos.withColumn("rank", F.row_number().over(video_window))

top_30_videos.show(31) #just to show all 30 rows when printing

# count frequency of comments containing the word "great" per video
great_in_comments = spark.sql("""
SELECT
  video_id,
  COUNT(comment_id) AS great_comment_count
FROM yt_comments_clean
WHERE comment_text LIKE '%great%'
GROUP BY video_id
ORDER BY great_comment_count DESC
""")
print("'Great' commnents Row count:", great_in_comments.count())
great_in_comments.show(10)

# count frequency of comments containing the word "great" per video
bad_in_comments = spark.sql("""
SELECT
  video_id,
  COUNT(comment_id) AS great_comment_count
FROM yt_comments_clean
WHERE comment_text LIKE '%bad%'
GROUP BY video_id
ORDER BY great_comment_count DESC
""")
print("'Bad' comments Row count:", bad_in_comments.count())
bad_in_comments.show(10)

# separate comment date and time
from pyspark.sql.functions import split, regexp_replace

comment_date_time = spark.sql("""
SELECT
  *
FROM yt_comments_clean
""")

comment_date_time = comment_date_time.withColumn("date", split(comment_date_time["published_at"], "T").getItem(0))
comment_date_time = comment_date_time.withColumn("time", regexp_replace(split(comment_date_time["published_at"], "T").getItem(1), "Z", ""))
comment_date_time = comment_date_time.drop("published_at")

comment_date_time.show(5)

# define sentiment keywords
negative_keywords = ['bad', 'hate', 'terrible', 'worst', 'poor', 'dislike', 'awful', 'negative', 'frustrating', 'annoying', 'disappointing', 'horrible', 'trash', 'sucks', 'fail', 'stupid', 'ridiculous', 'pathetic']
neutral_keywords = ['maybe', 'perhaps', 'also', 'and', 'but', 'or', 'so', 'then', 'just', 'yet', 'however', 'indeed', 'whether', 'meanwhile', 'regardless', 'either', 'neither', 'somewhat', 'a bit']
positive_keywords = ['good', 'love', 'great', 'best', 'excellent', 'amazing', 'fantastic', 'awesome', 'positive', 'happy', 'enjoy', 'like', 'super', 'wonderful', 'beautiful', 'perfect', 'brilliant', 'delightful', 'cool', 'nice']

print("Sentiment keyword lists defined.")

# filter comments by specific video
filtered_comments_df = comment_date_time.filter(comment_date_time.video_id == 'n_Lv_mw6m6c')
filtered_comments_df.show(5)

# determine sentiment of comment
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def classify_sentiment(comment):
    if comment is None:
        return "unknown"
    comment_lower = comment.lower()

    has_positive = any(keyword in comment_lower for keyword in positive_keywords)
    has_negative = any(keyword in comment_lower for keyword in negative_keywords)

    if has_positive and not has_negative:
        return "positive"
    elif has_negative and not has_positive:
        return "negative"
    elif not has_positive and not has_negative:

        has_neutral = any(keyword in comment_lower for keyword in neutral_keywords)
        if has_neutral:
            return "neutral"
        else:
            return "unknown"
    else:
        return "unknown"

sentiment_udf = udf(classify_sentiment, StringType())

sentiment_classified_df = filtered_comments_df.withColumn("sentiment", sentiment_udf(filtered_comments_df.comment_text))

sentiment_classified_df.show(5)

#user defined function for extracting keywords from comments
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, ArrayType, StructType

def extract_keywords(comment_text):
    if comment_text is None:
        return []

    comment_lower = comment_text.lower()

    found_negative = set()
    found_neutral = set()
    found_positive = set()

    for keyword in negative_keywords:
        if keyword in comment_lower:
            found_negative.add(keyword)

    for keyword in neutral_keywords:
        if keyword in comment_lower:
            found_neutral.add(keyword)

    for keyword in positive_keywords:
        if keyword in comment_lower:
            found_positive.add(keyword)

    result = []
    for keyword in found_negative:
        result.append((keyword, 'negative'))
    for keyword in found_neutral:
        result.append((keyword, 'neutral'))
    for keyword in found_positive:
        result.append((keyword, 'positive'))

    return result

keyword_sentiment_schema = ArrayType(StructType([
    StructField("keyword", StringType(), True),
    StructField("keyword_family", StringType(), True)
]))

extract_keywords_udf = udf(extract_keywords, ArrayType(StructType([
    StructField("keyword", StringType(), True),
    StructField("keyword_family", StringType(), True)
])))



print("\'extract_keywords\' function defined and registered as \'extract_keywords_udf\'.")

extracted_keywords_df = filtered_comments_df.withColumn("extracted_keywords", extract_keywords_udf(filtered_comments_df.comment_text))

extracted_keywords_df.printSchema()
extracted_keywords_df.show(5, truncate=False)

# create extracted keywords df
from pyspark.sql.functions import explode

keywords_extracted_df = sentiment_classified_df.withColumn(
    "extracted_keywords", extract_keywords_udf(sentiment_classified_df.comment_text)
)

exploded_keywords_df = keywords_extracted_df.withColumn(
    "exploded_keyword", explode("extracted_keywords")
).select(
    keywords_extracted_df["*"],
    F.col("exploded_keyword.keyword"),
    F.col("exploded_keyword.keyword_family")
).drop("extracted_keywords", "exploded_keyword")

exploded_keywords_df.printSchema()

exploded_keywords_df.show(5, truncate=False)

# count keyword occurences per comment
keyword_counts_df = exploded_keywords_df.groupBy("video_id", "keyword_family", "keyword").count()
keyword_counts_df = keyword_counts_df.orderBy(F.col("count").desc())

keyword_counts_df.show(10)
print("Row count:", keyword_counts_df.count())

# create keyword dataframe
all_keywords_data = []

for keyword in negative_keywords:
    all_keywords_data.append((keyword, 'negative'))

for keyword in neutral_keywords:
    all_keywords_data.append((keyword, 'neutral'))

for keyword in positive_keywords:
    all_keywords_data.append((keyword, 'positive'))

all_keywords_df = spark.createDataFrame(all_keywords_data, ["keyword", "keyword_family"])

all_keywords_df.show(10)
print("Row count:", all_keywords_df.count())

# displaying final results by joining the keyword families with extracted comments keywords, sorting the families and counts

from pyspark.sql.functions import lit

final_keyword_analysis_df = all_keywords_df.join(
    keyword_counts_df,
    on=["keyword", "keyword_family"],
    how="left_outer"
)

final_keyword_analysis_df = final_keyword_analysis_df.withColumn(
    "video_id",
    F.when(F.col("video_id").isNull(), lit("n_Lv_mw6m6c")).otherwise(F.col("video_id"))
).withColumn(
    "count",
    F.when(F.col("count").isNull(), lit(0)).otherwise(F.col("count"))
).orderBy(F.col("keyword_family"), F.col("count").desc())

final_keyword_analysis_df.show(60)
print("Row count:", final_keyword_analysis_df.count())



# calculate total counts for a video's positive/negative/neutral words
total_sentiment_counts = final_keyword_analysis_df.groupBy("video_id", "keyword_family").agg(
    F.sum("count").alias("total_category_count")
).orderBy("video_id", "keyword_family")

print("\nTotal counts of words by sentiment category for video 'n_Lv_mw6m6c':")
total_sentiment_counts.show()

# calculate the average number of occurrences of each keyword by category
avg_sentiment_keyword_occurrence = final_keyword_analysis_df.groupBy("video_id", "keyword_family").agg(
    F.round(F.avg("count"), 2).alias("avg_keyword_occurrence_in_category")
).orderBy("video_id", "keyword_family")

print("\nAverage occurrence of keywords by sentiment category for video 'n_Lv_mw6m6c':")
avg_sentiment_keyword_occurrence.show()

#temporal data: show the total comments per video and average number of comments per video each month

from pyspark.sql.functions import month, countDistinct

monthly_stats = cleaned_df.withColumn("month", month("published_at")) \
    .groupBy("month") \
    .agg(
        F.count("comment_id").alias("tmonth_total"),
        F.countDistinct("video_id").alias("total_videos_in_month")
    )

monthly_stats = monthly_stats.withColumn("tavg_per_video", F.round(F.col("tmonth_total") / F.col("total_videos_in_month"), 2)) \
    .filter(F.col("month").isNotNull())

monthly_stats = monthly_stats.orderBy("tmonth_total")

monthly_stats.orderBy("month").show()